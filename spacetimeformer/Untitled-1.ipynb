{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMN.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/pandas/io/parquet.py:493\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;129m@doc\u001b[39m(storage_options\u001b[38;5;241m=\u001b[39m_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_parquet\u001b[39m(\n\u001b[1;32m    430\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    437\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    438\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;124;03m    DataFrame\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m     impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m    496\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    497\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_nullable_dtypes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/pandas/io/parquet.py:60\u001b[0m, in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     58\u001b[0m             error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA suitable version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import the above resulted in these errors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('MN.parquet')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "argument --run_name: conflicting option string: --run_name",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mStock42\u001b[39;00m():\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodName \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m600x68a7sh-2.45S10.42L\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[1], line 634\u001b[0m, in \u001b[0;36mStock42\u001b[0;34m()\u001b[0m\n\u001b[1;32m    631\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 634\u001b[0m     parser \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m     args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args()\n\u001b[1;32m    636\u001b[0m     main(args)\n",
      "Cell \u001b[0;32mIn[1], line 523\u001b[0m, in \u001b[0;36mStock42.create_parser\u001b[0;34m()\u001b[0m\n\u001b[1;32m    521\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--attn_plot\u001b[39m\u001b[38;5;124m\"\u001b[39m, action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    522\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--debug\u001b[39m\u001b[38;5;124m\"\u001b[39m, action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 523\u001b[0m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_argument\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--run_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequired\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstocks_spatiotemporal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--accumulate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    525\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--val_check_interval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/spacetimeformer/lib/python3.8/argparse.py:1386\u001b[0m, in \u001b[0;36m_ActionsContainer.add_argument\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1384\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength of metavar tuple does not match nargs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/spacetimeformer/lib/python3.8/argparse.py:1749\u001b[0m, in \u001b[0;36mArgumentParser._add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_add_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m   1748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m action\u001b[38;5;241m.\u001b[39moption_strings:\n\u001b[0;32m-> 1749\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optionals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1751\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_positionals\u001b[38;5;241m.\u001b[39m_add_action(action)\n",
      "File \u001b[0;32m~/anaconda3/envs/spacetimeformer/lib/python3.8/argparse.py:1590\u001b[0m, in \u001b[0;36m_ArgumentGroup._add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_add_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m-> 1590\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_ArgumentGroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group_actions\u001b[38;5;241m.\u001b[39mappend(action)\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "File \u001b[0;32m~/anaconda3/envs/spacetimeformer/lib/python3.8/argparse.py:1400\u001b[0m, in \u001b[0;36m_ActionsContainer._add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_add_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m   1399\u001b[0m     \u001b[38;5;66;03m# resolve any conflicts\u001b[39;00m\n\u001b[0;32m-> 1400\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_conflict\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;66;03m# add to actions list\u001b[39;00m\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actions\u001b[38;5;241m.\u001b[39mappend(action)\n",
      "File \u001b[0;32m~/anaconda3/envs/spacetimeformer/lib/python3.8/argparse.py:1539\u001b[0m, in \u001b[0;36m_ActionsContainer._check_conflict\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m confl_optionals:\n\u001b[1;32m   1538\u001b[0m     conflict_handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_handler()\n\u001b[0;32m-> 1539\u001b[0m     \u001b[43mconflict_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfl_optionals\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/spacetimeformer/lib/python3.8/argparse.py:1548\u001b[0m, in \u001b[0;36m_ActionsContainer._handle_conflict_error\u001b[0;34m(self, action, conflicting_actions)\u001b[0m\n\u001b[1;32m   1542\u001b[0m message \u001b[38;5;241m=\u001b[39m ngettext(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconflicting option string: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1543\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconflicting option strings: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1544\u001b[0m                    \u001b[38;5;28mlen\u001b[39m(conflicting_actions))\n\u001b[1;32m   1545\u001b[0m conflict_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([option_string\n\u001b[1;32m   1546\u001b[0m                              \u001b[38;5;28;01mfor\u001b[39;00m option_string, action\n\u001b[1;32m   1547\u001b[0m                              \u001b[38;5;129;01min\u001b[39;00m conflicting_actions])\n\u001b[0;32m-> 1548\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ArgumentError(action, message \u001b[38;5;241m%\u001b[39m conflict_string)\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument --run_name: conflicting option string: --run_name"
     ]
    }
   ],
   "source": [
    "''' StockForecaster.py\n",
    "Class object to be run from terminal that performs the following operations:\n",
    "1. Obtain new data from IB\n",
    "2. Encode data to visual format\n",
    "3. Train new models (although this is typically done on GPU outside of this machine)\n",
    "4. Predict on specified models\n",
    "5. Grab recent data to make forecasts\n",
    "6. Identify most likely opportunities in options market \n",
    "7. Perform trades based on forecasts'''\n",
    "\n",
    "# Packages\n",
    "import warnings\n",
    "import os\n",
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from arch import arch_model  # !pip install arch==5.3.1\n",
    "import ta as ta  # !pip install git+https://github.com/bukosabino/ta.git\n",
    "import datetime as dt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ib_insync import *\n",
    "import pandas as pd\n",
    "# from keras import layers\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import sys\n",
    "import uuid\n",
    "\n",
    "import torch\n",
    "\n",
    "import spacetimeformer as stf\n",
    "from TimeSeriesDataset_ContextOnly import TimeSeriesDataset_ContextOnly\n",
    "from torch.utils.data import DataLoader\n",
    "import csv\n",
    "\n",
    "class Stock42():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.modName = '600x68a7sh-2.45S10.42L'\n",
    "        self.tix = ['AAPL','GOOG','TSLA','NFLX','DHR',\n",
    "            'MMM','PFE','AMZN','AMD','ABEV',\n",
    "            'NVDA','BAC','JPM','INTC','XOM',\n",
    "            'MSFT','C','CRM','BA','UNH',\n",
    "            'WMT','CVS','JNJ','V','LLY',  # This is the end of the original training set\n",
    "            'BRK B','AVGO','PG','MA','MRK',\n",
    "            'HD','CVX','PEP','MCD','CSCO',\n",
    "            'COST','TMO','ADBE','DIS','WFC',\n",
    "            'KR','MCK','T','CIG','CAH',\n",
    "            'ELV','MRO','WBA','VZ','PSX',\n",
    "            'UPS','DELL','LOW','ADM',\n",
    "            'GE','IBM','MET','PRU','RTX',\n",
    "            'HUM','COR','VLO','CNC','TJX',\n",
    "            'CMCSA','META','TGT','ET','FDX',\n",
    "            'AA','COP','ACI','GS',\n",
    "            'SYY','LMT','SNEX','MS','HP',\n",
    "            'CAT','HCA','DOW','ABBV','AIG',\n",
    "            'SNX','AXP','DAL','TSN','DE',\n",
    "            'PGR','NKE','BBY','PBF','PFGC',\n",
    "            'K','BMY','QCOM','ABT','UAL'\n",
    "            ]\n",
    "        self.featLength = 95\n",
    "        self.p = 10\n",
    "        self.trainDuration = '20 Y'\n",
    "        # self.dur = 10\n",
    "        self.InputLength = 252\n",
    "        self.bins = 10 # Also a default for histogram\n",
    "        self.extraBins = 10\n",
    "        self.lookback = [252] # [22, 65, 130, 22*9, 252] # M, Q, H, 3/4Y, Y\n",
    "        self.ul = 31.437988662719725\n",
    "        self.ll = -9.06309299468994\n",
    "        self.maxPositions=1 # Should be set as a global parameter\n",
    "        self.stpLsRatio = 0.7\n",
    "        self.profTakrRatio = 1.5\n",
    "        self.binDef = [1,2,5,10,15,20,30,65,130,252]\n",
    "\n",
    "\n",
    "    def LoadModelFromFile(self):\n",
    "        self.model = tf.keras.models.load_model(os.getcwd() + '/models/' + self.modName, compile=False) #140x_62a_10p_2sh\n",
    "\n",
    "    def predModel(self):\n",
    "        # Predict and provide new estimates\n",
    "        if self.ul == 0:\n",
    "            p = self.model.predict([self.supValDat[:,:,:,:],self.supValDat[:,-8:,:,:]])  ### Make a data attribute for supValDat & cur\n",
    "            self.ll = np.quantile(p,.1)\n",
    "            self.ul = np.quantile(p,.9)\n",
    "\n",
    "        preds = self.model.predict([self.curDat[:,:,:,:],self.curDat[:,-8:,:,:]])\n",
    "        j=0\n",
    "        self.curPreds = []#pd.DataFrame()\n",
    "        for t in self.tix:\n",
    "            if (preds[j,0] < self.ll) | (preds[j,0] > self.ul):\n",
    "                print([t,preds[j,0]])\n",
    "                self.curPreds.append([t,preds[j,0]])\n",
    "            j+=1\n",
    "        return\n",
    "\n",
    "    def loadData(self):\n",
    "        with open('./test_exp.npy', 'rb') as f:\n",
    "            self.supDat = np.load(f)\n",
    "            self.supTar = np.load(f)\n",
    "            self.supValDat = np.load(f)\n",
    "            self.supValTar = np.load(f)\n",
    "            self.curDat = np.load(f)\n",
    "\n",
    "    def connectIB(self):\n",
    "        util.startLoop()  # uncomment this line when in a notebook\n",
    "        self.ib = IB()\n",
    "        self.ib.disconnect()\n",
    "        self.ib.connect('127.0.0.1', 7496, clientId=11)\n",
    "        warnings.filterwarnings('ignore', category=RuntimeWarning, module='ib_insync')\n",
    "\n",
    "\n",
    "    def disconnectIB(self):\n",
    "        util.startLoop()  # uncomment this line when in a notebook\n",
    "        self.ib.disconnect()\n",
    "\n",
    "    def features(self, VIX, SPY, TNX):\n",
    "        warnings.filterwarnings('ignore')\n",
    "        periods=self.p\n",
    "        InputLength = self.InputLength\n",
    "        AllData = pd.DataFrame(self.AllData)\n",
    "        AllData[['VIX']] = VIX[['close']]\n",
    "        AllData[['SPY']] = SPY[['close']]\n",
    "        AllData[['TNX']] = TNX[['close']]   #####\n",
    "        AllData = AllData.reset_index(drop=True)\n",
    "        AllData = AllData.rename(columns={\"volume\": \"Volume\"})\n",
    "\n",
    "        rsi14 = ta.momentum.RSIIndicator(close=AllData['close'], window=14, fillna=True)\n",
    "        AllData['rsi14'] = rsi14.rsi()\n",
    "        rsi9 = ta.momentum.RSIIndicator(close=AllData['close'], window=9, fillna=True)\n",
    "        AllData['rsi9'] = rsi9.rsi()\n",
    "        rsi24 = ta.momentum.RSIIndicator(close=AllData['close'], window=24, fillna=True)\n",
    "        AllData['rsi24'] = rsi24.rsi()\n",
    "        # # MACD 5, 35, 5\n",
    "        macd5355 = ta.trend.MACD(close=AllData['close'], window_slow=35, window_fast=5, window_sign=5, fillna=True)\n",
    "        AllData['MACD5355macddiff'] = macd5355.macd_diff()\n",
    "        AllData['MACD5355macddiffslope'] = macd5355.macd_diff().diff() # Slope of line\n",
    "        AllData['MACD5355macd'] = macd5355.macd()\n",
    "        AllData['MACD5355macdslope'] = macd5355.macd().diff()\n",
    "        AllData['MACD5355macdsig'] = macd5355.macd_signal()\n",
    "        AllData['MACD5355macdsigslope'] = macd5355.macd_signal().diff()\n",
    "\n",
    "        # MACD 12,26,9\n",
    "        macd12269 = ta.trend.MACD(close=AllData['close'], window_slow=26, window_fast=12, window_sign=9, fillna=True)\n",
    "        AllData['MACD12269macddiff'] = macd12269.macd_diff()\n",
    "        AllData['MACD12269macddiffslope'] = macd12269.macd_diff().diff() # Slope of line\n",
    "        AllData['MACD12269macd'] = macd12269.macd()\n",
    "        AllData['MACD12269macdslope'] = macd12269.macd().diff()\n",
    "        AllData['MACD12269macdsig'] = macd12269.macd_signal()\n",
    "        AllData['MACD12269macdsigslope'] = macd12269.macd_signal().diff()\n",
    "\n",
    "        # Engulfing Bars\n",
    "        AllData['lowTail'] = AllData['low'].pct_change()\n",
    "        AllData['highTail'] = AllData['high'].pct_change()\n",
    "        AllData['openTail'] = AllData['open'].pct_change()\n",
    "        AllData['IntradayBar'] = (AllData.close.values[:] - AllData.open.values[:]) / AllData.open.values[:]\n",
    "        AllData['IntradayRange'] = (AllData.high.values[:] - AllData.low.values[:]) / AllData.low.values[:]\n",
    "        # SMA divergence\n",
    "        for s in [5,10, 12 ,20, 30,65 ,50,100, 200]:\n",
    "            AllData['CloseOverSMA'+str(s)] = AllData['close'] / AllData['close'].rolling(window=s).mean()\n",
    "\n",
    "        # SMA Volume divergence\n",
    "        for s in [5,10, 12 ,20, 30,65 ,50,100, 200]:\n",
    "            AllData['VolOverSMA'+str(s)] = AllData['Volume'] / AllData['Volume'].rolling(window=s).mean()\n",
    "\n",
    "        # Recent Performance\n",
    "        AllData['Ret1day'] = AllData['close'].pct_change(1)  # 1\n",
    "        AllData['Ret4day'] = AllData['close'].pct_change(5)  # 4\n",
    "        AllData['Ret8day'] = AllData['close'].pct_change(10)  # 8\n",
    "        AllData['Ret12day'] = AllData['close'].pct_change(15)# 12\n",
    "        AllData['Ret24day'] = AllData['close'].pct_change(20)#24\n",
    "        AllData['Ret72day'] = AllData['close'].pct_change(65)#72\n",
    "        AllData['Ret240day'] = AllData['close'].pct_change(252)#240\n",
    "\n",
    "        # Relative Strength Comparison w/ SPY\n",
    "\n",
    "        AllData['RSC'] = AllData['Ret1day']-AllData['SPY'].pct_change(1)\n",
    "\n",
    "        # BBs\n",
    "        BB = ta.volatility.BollingerBands(close=AllData['close'], fillna=True, window=22)\n",
    "        AllData['bands_l'] = BB.bollinger_lband()\n",
    "        AllData['bands_u'] = BB.bollinger_hband()\n",
    "\n",
    "        # ADX\n",
    "        ADX = ta.trend.ADXIndicator(high=AllData['high'], low=AllData['low'], close=AllData['close'], window=14, fillna=True)\n",
    "        AllData['ADX'] = ADX.adx()\n",
    "\n",
    "        # # Ichimoku\n",
    "        ICH = ta.trend.IchimokuIndicator(high=AllData['high'], low=AllData['low'], fillna=True)\n",
    "        AllData['cloudA'] = ICH.ichimoku_a()\n",
    "        AllData['cloudB'] = ICH.ichimoku_b()\n",
    "        AllData['closeVsIchA'] = AllData['close'] / ICH.ichimoku_a()\n",
    "        AllData['closeVsIchB'] = AllData['close'] / ICH.ichimoku_b()\n",
    "        AllData['IchAvIchB'] = ICH.ichimoku_a() / ICH.ichimoku_b()\n",
    "\n",
    "        # # Garch volatility forecast\n",
    "\n",
    "        AllData = AllData.dropna(axis=0,how='any')\n",
    "\n",
    "        am = arch_model(100*AllData['Ret1day'])\n",
    "        res = am.fit(update_freq=1, disp=\"off\", show_warning=False)\n",
    "        AllData['CondVol_1'] = res.conditional_volatility.values.reshape(-1,1)\n",
    "\n",
    "        am = arch_model(100*AllData['Ret4day']) # 4\n",
    "        res = am.fit(update_freq=1, disp=\"off\", show_warning=False)\n",
    "        AllData['CondVol_4'] = res.conditional_volatility.values.reshape(-1,1)\n",
    "\n",
    "        am = arch_model(100*AllData['Ret8day']) # 8\n",
    "        res = am.fit(update_freq=1, disp=\"off\", show_warning=False)\n",
    "        AllData['CondVol_8'] = res.conditional_volatility.values.reshape(-1,1)\n",
    "\n",
    "        am = arch_model(100*AllData['Ret12day']) # 12\n",
    "        res = am.fit(update_freq=1, disp=\"off\", show_warning=False)\n",
    "        AllData['CondVol_12'] = res.conditional_volatility.values.reshape(-1,1)\n",
    "\n",
    "        am = arch_model(100*AllData['Ret24day']) # 24\n",
    "        res = am.fit(update_freq=1, disp=\"off\", show_warning=False)\n",
    "        AllData['CondVol_24'] = res.conditional_volatility.values.reshape(-1,1)\n",
    "\n",
    "        am = arch_model(100*AllData['Ret72day']) # 72\n",
    "        res = am.fit(update_freq=1, disp=\"off\", show_warning=False)\n",
    "        AllData['CondVol_72'] = res.conditional_volatility.values.reshape(-1,1)\n",
    "\n",
    "        am = arch_model(100*AllData['Ret240day']) # 240\n",
    "        res = am.fit(update_freq=1, disp=\"off\", show_warning=False)\n",
    "        AllData['CondVol_240'] = res.conditional_volatility.values.reshape(-1,1)\n",
    "\n",
    "        AllData['CV1vCV4'] = AllData['CondVol_1']-AllData['CondVol_4']\n",
    "        AllData['CV4vCV8'] = AllData['CondVol_4']-AllData['CondVol_8']\n",
    "        AllData['CV8vCV12'] = AllData['CondVol_8']-AllData['CondVol_12']\n",
    "        AllData['CV12vCV24'] = AllData['CondVol_12']-AllData['CondVol_24']\n",
    "        AllData['CV8vCV24'] = AllData['CondVol_8']-AllData['CondVol_24']\n",
    "        AllData['CV24vCV240'] = AllData['CondVol_24'] - AllData['CondVol_240']\n",
    "\n",
    "        AllData['RSC_VIX'] = AllData['CondVol_1']-AllData['VIX'].pct_change(1)\n",
    "        AllData['RSC_VIX_IV'] = AllData['vclose']-AllData['VIX'].pct_change(1)\n",
    "        AllData['RSC_VIX_real'] = AllData['CondVol_1']-AllData['VIX']\n",
    "        AllData['RSC_VIX_IV_real'] = AllData['vclose']-AllData['VIX']\n",
    "        AllData['RSC_IV_gar'] = AllData['vclose']-AllData['CondVol_1']\n",
    "\n",
    "        AllData['close_spy_corr22'] = AllData['close'].rolling(22).corr(AllData['SPY'])\n",
    "        AllData['close_tnx_corr22'] = AllData['close'].rolling(22).corr(AllData['TNX'])\n",
    "        AllData['vclose_VIX_corr22'] = AllData['vclose'].rolling(22).corr(AllData['VIX'])\n",
    "        AllData['garch_IV_corr22'] = AllData['CondVol_1'].rolling(22).corr(AllData['vclose'])\n",
    "\n",
    "        AllData['close_spy_corr65'] = AllData['close'].rolling(65).corr(AllData['SPY'])\n",
    "        AllData['close_tnx_corr65'] = AllData['close'].rolling(65).corr(AllData['TNX'])\n",
    "        AllData['vclose_VIX_corr65'] = AllData['vclose'].rolling(65).corr(AllData['VIX'])\n",
    "        AllData['garch_IV_corr65'] = AllData['CondVol_1'].rolling(65).corr(AllData['vclose'])\n",
    "\n",
    "        AllData['close_spy_corr252'] = AllData['close'].rolling(252).corr(AllData['SPY'])\n",
    "        AllData['close_tnx_corr252'] = AllData['close'].rolling(252).corr(AllData['TNX'])\n",
    "        AllData['vclose_VIX_corr252'] = AllData['vclose'].rolling(252).corr(AllData['VIX'])\n",
    "        AllData['garch_IV_corr252'] = AllData['CondVol_1'].rolling(252).corr(AllData['vclose'])\n",
    "\n",
    "        # Clean up output\n",
    "        AllData = AllData.dropna(axis=0, how='any')\n",
    "        AllData = AllData.drop(columns=['Volume'], axis=1)\n",
    "        AllData = AllData.rename(columns={\"close\": \"Close\"}) # Parameter assignment for RL target\n",
    "\n",
    "        tar = np.nan * (np.zeros(len(AllData.Close)))\n",
    "        tar[:-periods] = AllData.Close.values[periods:] / AllData.open.values[1:len(AllData.Close) - periods + 1] - 1\n",
    "        AllData['WasUp'] = tar\n",
    "        self.AllData = AllData\n",
    "        return\n",
    "\n",
    "    def getDat(self,symbol):\n",
    "        contract = Stock(symbol, 'SMART', 'USD')\n",
    "        self.ib.qualifyContracts(contract)\n",
    "        # Hist OLHCV data\n",
    "        self.historical_data = self.ib.reqHistoricalData(\n",
    "            contract, \n",
    "            endDateTime='',\n",
    "            barSizeSetting='1 day', \n",
    "            durationStr=self.trainDuration, \n",
    "            whatToShow='ADJUSTED_LAST',\n",
    "            useRTH=True,\n",
    "            )\n",
    "        AllData = util.df(self.historical_data)\n",
    "        AllData=AllData.set_index(AllData['date'],drop=True)\n",
    "        # Volatility\n",
    "        IV_historical_data = self.ib.reqHistoricalData(\n",
    "            contract, \n",
    "            endDateTime='',\n",
    "            barSizeSetting='1 day', \n",
    "            durationStr=self.trainDuration, \n",
    "            whatToShow='OPTION_IMPLIED_VOLATILITY',\n",
    "            useRTH=True,\n",
    "            )\n",
    "        AD_IV = util.df(IV_historical_data)\n",
    "        AD_IV=AD_IV.set_index(AD_IV['date'], drop=True)\n",
    "        AD_IV=AD_IV.drop(columns=['barCount','volume'])\n",
    "        AD_IV=AD_IV.rename(columns={'open':'vopen','high':'vhigh','low':'vlow','close':'vclose','average':'vaverage'})\n",
    "        AllData[['vclose','vopen','vhigh','vlow','vaverage']] = AD_IV[['vclose','vopen','vhigh','vlow','vaverage']]\n",
    "        AllData=AllData.drop(columns=['date','barCount','vaverage','average']) # Drop average and vaverage because these are not accessible in live data\n",
    "        self.AllData = AllData\n",
    "        return\n",
    "    \n",
    "    def GetLastTick(self,ticker):\n",
    "        # contract = Stock('AAPL', 'SMART', 'USD')\n",
    "        contract = Stock(ticker, 'SMART', 'USD')\n",
    "        self.ib.qualifyContracts(contract)\n",
    "        self.temp = self.ib.reqMktData(contract)\n",
    "        # print('O', self.temp.open,\n",
    "        #         'H', self.temp.high, \n",
    "        #         'L', self.temp.low,\n",
    "        #         'Last', self.temp.last)\n",
    "\n",
    "    def GetCurrentData(self):\n",
    "        # Index & VIX data\n",
    "        contract = Index('SPX', 'CBOE', 'USD')\n",
    "        self.ib.qualifyContracts(contract)\n",
    "        historical_data = self.ib.reqHistoricalData(\n",
    "            contract, \n",
    "            endDateTime='',\n",
    "            barSizeSetting='1 day', \n",
    "            durationStr=self.trainDuration, \n",
    "            whatToShow='ADJUSTED_LAST',\n",
    "            useRTH=True,\n",
    "            )\n",
    "        SPY = util.df(historical_data)\n",
    "        SPY=SPY.set_index(SPY['date'], drop=True)\n",
    "\n",
    "        contract = Index('VIX', 'CBOE', 'USD')\n",
    "        self.ib.qualifyContracts(contract)\n",
    "        historical_data = self.ib.reqHistoricalData(\n",
    "            contract, \n",
    "            endDateTime='',\n",
    "            barSizeSetting='1 day', \n",
    "            durationStr=self.trainDuration, \n",
    "            whatToShow='ADJUSTED_LAST',\n",
    "            useRTH=True,\n",
    "            )\n",
    "        VIX = util.df(historical_data)\n",
    "        VIX=VIX.set_index(VIX['date'], drop=True)\n",
    "        # New treasury linked features\n",
    "        contract = Index('TNX', 'CBOE', 'USD')\n",
    "        self.ib.qualifyContracts(contract)\n",
    "        historical_data = self.ib.reqHistoricalData(\n",
    "            contract, \n",
    "            endDateTime='',\n",
    "            barSizeSetting='1 day', \n",
    "            durationStr=self.trainDuration, \n",
    "            whatToShow='ADJUSTED_LAST',\n",
    "            useRTH=True,\n",
    "            )\n",
    "        TNX = util.df(historical_data)\n",
    "        TNX=TNX.set_index(TNX['date'], drop=True)\n",
    "\n",
    "        print('Getting Data for equities...')\n",
    "        self.MuSigTix = pd.DataFrame(columns=['ticker','closemu','closesig','volmu','volsig'])\n",
    "\n",
    "        for t in self.tix:\n",
    "            # print(t)\n",
    "            self.getDat(t)\n",
    "            self.features(VIX, SPY, TNX)\n",
    "            self.AllData = self.AllData.drop(columns=['WasUp'])\n",
    "            for f in range(len(self.AllData.columns)): # features\n",
    "                # should have two classes. 1 for all non-target features that is normal scaled\n",
    "                # the other that has the target variables that are not scaled or min/max scaled.\n",
    "                feat = self.AllData.columns[f]\n",
    "                rollmu = self.AllData[feat].rolling(252).mean()\n",
    "                rollstd = self.AllData[feat].rolling(252).std()\n",
    "                self.AllData[feat]=(self.AllData[feat]-rollmu)/rollstd\n",
    "                if feat == 'Close':\n",
    "                    closemu = rollmu\n",
    "                    closestd = rollstd\n",
    "                if feat == 'vclose':\n",
    "                    volmu = rollmu\n",
    "                    volstd = rollstd\n",
    "\n",
    "            new_row = {'ticker': t, 'closemu': closemu.iloc[-1], 'closesig': closestd.iloc[-1],\n",
    "                       'volmu': volmu.iloc[-1], 'volsig': volstd.iloc[-1]}\n",
    "            self.MuSigTix = self.MuSigTix.append(new_row, ignore_index=True)\n",
    "                # if feat not in [\"Close\", \"vclose\"]:\n",
    "                #     self.AllData[feat]=(self.AllData[feat]-self.AllData[feat].rolling(252).mean())/self.AllData[feat].rolling(252).std()\n",
    "                # else: # min max scaling\n",
    "                #     self.AllData[feat]=(self.AllData[feat]-self.AllData[feat].rolling(252).min())/(self.AllData[feat].rolling(252).max()-self.AllData[feat].rolling(252).min())\n",
    "\n",
    "            self.AllData=self.AllData.dropna(axis=0)\n",
    "            self.AllData=self.AllData.reset_index(drop=True)\n",
    "            oos_len = 252  #### Should be user defined (oos, tr, etc...)\n",
    "            tr_len = int(.75*len(self.AllData[:-oos_len]))\n",
    "            self.AllData[:tr_len].to_csv('./data/train/' + t + '.csv')\n",
    "            self.AllData[tr_len:-oos_len].to_csv('./data/test/' + t + '.csv')\n",
    "            self.AllData[-oos_len:].to_csv('./data/oos/' + t + '.csv')\n",
    "        self.MuSigTix.to_csv('./data/TixMuSig.csv')\n",
    "        return\n",
    "\n",
    "    _MODELS = [\"spacetimeformer\"]\n",
    "    _DSETS = [\"stocks\"]\n",
    "\n",
    "    def create_model(config):\n",
    "        x_dim, yc_dim, yt_dim = None, None, None\n",
    "        if config.dset == \"stocks\":\n",
    "            x_dim = 95\n",
    "            yc_dim = 2 # Can reduce to specific features. i.e you could forecast only 'Close' (yc_dim=1)\n",
    "            yt_dim = 2\n",
    "\n",
    "        assert x_dim is not None\n",
    "        assert yc_dim is not None\n",
    "        assert yt_dim is not None\n",
    "\n",
    "        if config.model == \"spacetimeformer\":\n",
    "            if hasattr(config, \"context_points\") and hasattr(config, \"target_points\"):\n",
    "                max_seq_len = config.context_points + config.target_points\n",
    "            elif hasattr(config, \"max_len\"):\n",
    "                max_seq_len = config.max_len\n",
    "            else:\n",
    "                raise ValueError(\"Undefined max_seq_len\")\n",
    "            forecaster = stf.spacetimeformer_model.Spacetimeformer_Forecaster(\n",
    "                d_x=x_dim,\n",
    "                d_yc=yc_dim,\n",
    "                d_yt=yt_dim,\n",
    "                max_seq_len=max_seq_len,\n",
    "                start_token_len=config.start_token_len,\n",
    "                attn_factor=config.attn_factor,\n",
    "                d_model=config.d_model,\n",
    "                d_queries_keys=config.d_qk,\n",
    "                d_values=config.d_v,\n",
    "                n_heads=config.n_heads,\n",
    "                e_layers=config.enc_layers,\n",
    "                d_layers=config.dec_layers,\n",
    "                d_ff=config.d_ff,\n",
    "                dropout_emb=config.dropout_emb,\n",
    "                dropout_attn_out=config.dropout_attn_out,\n",
    "                dropout_attn_matrix=config.dropout_attn_matrix,\n",
    "                dropout_qkv=config.dropout_qkv,\n",
    "                dropout_ff=config.dropout_ff,\n",
    "                pos_emb_type=config.pos_emb_type,\n",
    "                use_final_norm=not config.no_final_norm,\n",
    "                global_self_attn=config.global_self_attn,\n",
    "                local_self_attn=config.local_self_attn,\n",
    "                global_cross_attn=config.global_cross_attn,\n",
    "                local_cross_attn=config.local_cross_attn,\n",
    "                performer_kernel=config.performer_kernel,\n",
    "                performer_redraw_interval=config.performer_redraw_interval,\n",
    "                attn_time_windows=config.attn_time_windows,\n",
    "                use_shifted_time_windows=config.use_shifted_time_windows,\n",
    "                norm=config.norm,\n",
    "                activation=config.activation,\n",
    "                init_lr=config.init_lr,\n",
    "                base_lr=config.base_lr,\n",
    "                warmup_steps=config.warmup_steps,\n",
    "                decay_factor=config.decay_factor,\n",
    "                initial_downsample_convs=config.initial_downsample_convs,\n",
    "                intermediate_downsample_convs=config.intermediate_downsample_convs,\n",
    "                embed_method=config.embed_method,\n",
    "                l2_coeff=config.l2_coeff,\n",
    "                loss=config.loss,\n",
    "                class_loss_imp=config.class_loss_imp,\n",
    "                recon_loss_imp=config.recon_loss_imp,\n",
    "                time_emb_dim=config.time_emb_dim,\n",
    "                null_value=config.null_value,\n",
    "                pad_value=config.pad_value,\n",
    "                linear_window=config.linear_window,\n",
    "                use_revin=config.use_revin,\n",
    "                linear_shared_weights=config.linear_shared_weights,\n",
    "                use_seasonal_decomp=config.use_seasonal_decomp,\n",
    "                use_val=not config.no_val,\n",
    "                use_time=not config.no_time,\n",
    "                use_space=not config.no_space,\n",
    "                use_given=not config.no_given,\n",
    "                recon_mask_skip_all=config.recon_mask_skip_all,\n",
    "                recon_mask_max_seq_len=config.recon_mask_max_seq_len,\n",
    "                recon_mask_drop_seq=config.recon_mask_drop_seq,\n",
    "                recon_mask_drop_standard=config.recon_mask_drop_standard,\n",
    "                recon_mask_drop_full=config.recon_mask_drop_full,\n",
    "            )\n",
    "        return forecaster\n",
    "\n",
    "    def create_parser():\n",
    "        default_model = \"spacetimeformer\"\n",
    "        default_dset = \"stocks\"\n",
    "        \n",
    "        model = sys.argv[1] if len(sys.argv) > 1 else default_model\n",
    "        dset = sys.argv[2] if len(sys.argv) > 2 else default_dset\n",
    "        context_points=100\n",
    "        target_points=10\n",
    "        epochs=200\n",
    "        run_name=\"stocks_spatiotemporal\"\n",
    "\n",
    "        # model = sys.argv[1]\n",
    "        # dset = sys.argv[2]\n",
    "        # Throw error now before we get confusing parser issues\n",
    "        # assert (\n",
    "        #     model in _MODELS\n",
    "        # ), f\"Unrecognized model (`{model}`). Options include: {_MODELS}\"\n",
    "        # assert dset in _DSETS, f\"Unrecognized dset (`{dset}`). Options include: {_DSETS}\"\n",
    "\n",
    "        parser = ArgumentParser()\n",
    "        parser.add_argument(\"model\")\n",
    "        parser.add_argument(\"dset\")\n",
    "        parser.add_argument(\"context_points\")\n",
    "        parser.add_argument(\"target_points\")\n",
    "        parser.add_argument(\"epochs\")\n",
    "        parser.add_argument(\"run_name\")\n",
    "        \n",
    "\n",
    "        if dset == \"stocks\":\n",
    "            parser.add_argument(\"--train_data_path\", type=str, default=\"spacetimeformer_stocks/data/train\",\n",
    "                                help=\"Path to the training data for the 'stocks' dataset\")\n",
    "            parser.add_argument(\"--test_data_path\", type=str, default=\"spacetimeformer_stocks/data/test\",\n",
    "                                help=\"Path to the test data for the 'stocks' dataset\")\n",
    "            parser.add_argument(\"--oos_data_path\", type=str, default=\"spacetimeformer_stocks/data/oos\",\n",
    "                                help=\"Path to the out-of-sample data for the 'stocks' dataset\")\n",
    "            # parser.add_argument(\"--context_points\", type=int, required=True, help=\"Number of context points\")\n",
    "            # parser.add_argument(\"--target_points\", type=int, required=True, help=\"Number of target points to predict\")\n",
    "            # parser.add_argument(\"--epochs\", type=int, required=True, help=\"Number of training epochs\")\n",
    "        stf.data.DataModule.add_cli(parser)\n",
    "\n",
    "        if model == \"spacetimeformer\":\n",
    "            stf.spacetimeformer_model.Spacetimeformer_Forecaster.add_cli(parser)\n",
    "        stf.callbacks.TimeMaskedLossCallback.add_cli(parser)\n",
    "\n",
    "        parser.add_argument(\"--wandb\", action=\"store_true\")\n",
    "        parser.add_argument(\"--plot\", action=\"store_true\")\n",
    "        parser.add_argument(\"--plot_samples\", type=int, default=8)\n",
    "        parser.add_argument(\"--attn_plot\", action=\"store_true\")\n",
    "        parser.add_argument(\"--debug\", action=\"store_true\")\n",
    "        parser.add_argument(\"--run_name\", type=str, required=True, default = \"stocks_spatiotemporal\")\n",
    "        parser.add_argument(\"--accumulate\", type=int, default=1)\n",
    "        parser.add_argument(\"--val_check_interval\", type=float, default=1.0)\n",
    "        parser.add_argument(\"--limit_val_batches\", type=float, default=1.0)\n",
    "        parser.add_argument(\"--no_earlystopping\", action=\"store_true\")\n",
    "        parser.add_argument(\"--patience\", type=int, default=5)\n",
    "        parser.add_argument(\n",
    "            \"--trials\", type=int, default=1, help=\"How many consecutive trials to run\"\n",
    "        )\n",
    "        if len(sys.argv) > 3 and sys.argv[3] == \"-h\":\n",
    "            parser.print_help()\n",
    "            sys.exit(0)\n",
    "        return parser\n",
    "\n",
    "    def main(args):\n",
    "        # Initialization and Setup\n",
    "        log_dir = os.getenv(\"STF_LOG_DIR\", \"./data/STF_LOG_DIR\")\n",
    "        args.use_gpu = False\n",
    "        device = torch.device(\"cpu\")\n",
    "        # device = torch.device(\"cuda\" if torch.cuda.is_available() and args.use_gpu else \"cpu\")\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "\n",
    "        if args.wandb:\n",
    "            import wandb\n",
    "            project = os.getenv(\"STF_WANDB_PROJ\")\n",
    "            entity = os.getenv(\"STF_WANDB_ACCT\")\n",
    "            experiment = wandb.init(project=project, entity=entity, config=args, dir=log_dir, reinit=True)\n",
    "            config = wandb.config\n",
    "            wandb.run.name = args.run_name\n",
    "            wandb.run.save()\n",
    "            logger = pl.loggers.WandbLogger(experiment=experiment, save_dir=log_dir)\n",
    "\n",
    "        # Data Preparation\n",
    "        if args.dset == \"stocks\":\n",
    "            print('FFUUUUCK')\n",
    "            # Custom DataLoader for 'stocks'\n",
    "            args.null_value = None # NULL_VAL\n",
    "            args.pad_value = None\n",
    "\n",
    "            folder='spacetimeformer/data/oos'\n",
    "            xt_holder = []  # Initialize xt_holder as an empty list to hold tensors\n",
    "            for i in os.listdir(folder):  # loop over data in the oos folder by ticker symbol\n",
    "                dataset = TimeSeriesDataset_ContextOnly(folder_name=folder, file_name=i, context_length=args.context_points)\n",
    "                dataloader = DataLoader(dataset, batch_size=1000, shuffle=False)  # Batch size of 1000 ensures all data is in one batch\n",
    "                for batch_idx, (context) in enumerate(dataloader):\n",
    "                    # Unpack batch into x_c, y_c, x_t, y_t\n",
    "                    x_t = context[:, -args.context_points:, :]  # Context features\n",
    "                    xt_holder.append(x_t[-1,:,:])  # Append the last item of x_t to xt_holder\n",
    "\n",
    "            # Ensure torch.stack() is called outside the loop, after xt_holder has collected all tensors\n",
    "            xt_holder = torch.stack(xt_holder, dim=0)\n",
    "            print('Eval Dataset Shape: ', xt_holder.shape)\n",
    "\n",
    "        # Model Training and Evaluation\n",
    "        forecaster = create_model(args)\n",
    "        forecaster = forecaster.to(device)  # Move the model to the specified device\n",
    "    # 2/24\n",
    "        output_path = \"/Users/alecjeffery/Documents/Playgrounds/Python/largeModels/feb24_2024.pth\"\n",
    "\n",
    "        # Load the weights into the model\n",
    "        # forecaster.load_state_dict(torch.load(output_path))\n",
    "        forecaster.load_state_dict(torch.load(output_path, map_location=torch.device('cpu')))\n",
    "\n",
    "        stock_names = [i[:-4] for i in os.listdir(folder)]  # Extract stock names from filenames\n",
    "        print('STOCK NAMED',stock_names)\n",
    "        if args.dset == \"stocks\":\n",
    "            forecaster.eval()\n",
    "            with torch.no_grad():\n",
    "                x_c = xt_holder[:, args.target_points:, :]\n",
    "                y_c = xt_holder[:, args.target_points:, [3, 4]]\n",
    "                x_t = xt_holder[:, -args.target_points:, :]  # Assuming x_t is used for prediction\n",
    "                y_t = xt_holder[:, -args.target_points:, [3, 4]]\n",
    "\n",
    "                x_c, y_c, x_t, y_t = x_c.to(device), y_c.to(device), x_t.to(device), y_t.to(device)\n",
    "                model_output = forecaster(x_c, y_c, x_t, y_t)\n",
    "                \n",
    "                predictions = model_output[0] if isinstance(model_output, tuple) else model_output\n",
    "                predictions = predictions.cpu().detach().numpy()  # Move to CPU and convert to numpy\n",
    "\n",
    "                # Separate 'close' and 'volatility' values\n",
    "                close_values = predictions[:, :, 0]  # Assuming 'close' values are the first in the last dimension\n",
    "                volatility_values = predictions[:, :, 1]  # Assuming 'volatility' values are the second\n",
    "\n",
    "                # Flatten 'close' and 'volatility' arrays\n",
    "                close_flattened = close_values.reshape(predictions.shape[0], -1)\n",
    "                volatility_flattened = volatility_values.reshape(predictions.shape[0], -1)\n",
    "\n",
    "                # Concatenate the flattened 'close' and 'volatility' arrays horizontally\n",
    "                predictions_flattened = np.hstack((close_flattened, volatility_flattened))\n",
    "\n",
    "                # Create column names for the DataFrame\n",
    "                close_columns = [f'Close_{i+1}' for i in range(close_values.shape[1])]\n",
    "                volatility_columns = [f'Volatility_{i+1}' for i in range(volatility_values.shape[1])]\n",
    "                column_names = close_columns + volatility_columns\n",
    "\n",
    "                # Assuming each sample's predictions are now correctly ordered and flattened\n",
    "                if len(predictions_flattened) == len(stock_names):\n",
    "                    # Create the DataFrame with the reshaped predictions\n",
    "                    predictions_df = pd.DataFrame(predictions_flattened, columns=column_names, index=stock_names)\n",
    "\n",
    "                    # Save to CSV with stock names as row indices\n",
    "                    predictions_df.to_csv('oos_predictions.csv')\n",
    "                else:\n",
    "                    print(\"Mismatch between the number of predictions and the number of stock names.\")\n",
    "\n",
    "        # WANDB Experiment Finish (if applicable)\n",
    "        if args.wandb:\n",
    "            wandb.finish()\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        parser = create_parser()\n",
    "        args = parser.parse_args()\n",
    "        main(args)\n",
    "    \n",
    "    def getOptionPositions(self):\n",
    "        # self.connectIB()\n",
    "        self.things = {}\n",
    "        for p in range(len(self.curPreds)):\n",
    "            sym = self.curPreds[p][0]\n",
    "            contract = Stock(sym, 'SMART','USD')\n",
    "            self.ib.qualifyContracts(contract)\n",
    "\n",
    "            self.ib.reqMarketDataType(4)\n",
    "            [ticker] = self.ib.reqTickers(contract)\n",
    "            print(ticker)\n",
    "\n",
    "            chains = self.ib.reqSecDefOptParams(contract.symbol, '', contract.secType, contract.conId)\n",
    "            chain = next(c for c in chains if c.tradingClass == sym and c.exchange == 'SMART')\n",
    "\n",
    "            strikes = [strike for strike in chain.strikes\n",
    "                    if strike % 1 == 0\n",
    "                    and ticker.last*.7 < strike < ticker.last*1.3]\n",
    "            # strikes = [strike for strike in chain.strikes\n",
    "            #         if ticker.last*.9 < strike < ticker.last*1.1]\n",
    "\n",
    "            expirations = sorted(exp for exp in chain.expirations)[3:4] # The next 4 wkly option expirations \n",
    "            # NEED TO INDICATE WHETHER C OR P BASED ON THE PREDICTED VALUE\n",
    "            # rights = ['P', 'C']\n",
    "            if self.curPreds[p][1] > 0:\n",
    "                rights = ['C']\n",
    "            else:\n",
    "                rights = ['P']\n",
    "\n",
    "            contracts = [Option(sym, expiration, strike, right, 'SMART', tradingClass=sym)\n",
    "                    for right in rights\n",
    "                    for expiration in expirations\n",
    "                    for strike in strikes]\n",
    "\n",
    "            contracts = self.ib.qualifyContracts(*contracts)\n",
    "\n",
    "            # datetime.today().strftime('%Y%m%d')\n",
    "\n",
    "            def days_between(exp):\n",
    "                d1 = datetime.strptime(datetime.today().strftime('%Y%m%d'), \"%Y%m%d\")\n",
    "                d2 = datetime.strptime(exp, \"%Y%m%d\")\n",
    "                return abs((d2 - d1).days)\n",
    "\n",
    "            dur = []\n",
    "            for i in range(len(expirations)):\n",
    "                dur.append(days_between(expirations[i]))\n",
    "\n",
    "            try: # sometimes this logic breaks when the strikes are sparse or nonexistent\n",
    "                strikes[min(range(len(strikes)), key = lambda i: abs(strikes[i]-ticker.last))]\n",
    "                strikes = np.array(strikes)\n",
    "                strike_loc = np.where(strikes == strikes[min(range(len(strikes)), key = lambda i: abs(strikes[i]-ticker.last))]) # Finds closest strike\n",
    "\n",
    "                dur = np.array(dur)\n",
    "                exp_loc = np.where(dur == dur[min(range(len(dur)), key = lambda i: abs(dur[i]-22))])\n",
    "                exp_loc = exp_loc[0][0]\n",
    "                strike_loc = strike_loc[[0][0]]\n",
    "\n",
    "                # print('strike: ',strikes[strike_loc][0])\n",
    "                # print('expiration: ',expirations[exp_loc])\n",
    "                # need to swap out the 'C' below in favor of rights\n",
    "                # contract = Option(sym, str(expirations[exp_loc]), strikes[strike_loc][0], 'C', 'SMART')\n",
    "                contract = Option(sym, str(expirations[exp_loc]), strikes[strike_loc][0], rights[0], 'SMART')\n",
    "                contract = self.ib.qualifyContracts(contract)[0]  # To fill in other contract details\n",
    "                details = self.ib.reqTickers(contract,)\n",
    "\n",
    "                # print(details)\n",
    "                # print ('Bid: ',details[0].bid)\n",
    "                # print ('Ask: ',details[0].ask)\n",
    "                # print ('Delta: ',details[0].lastGreeks.delta)\n",
    "                # print ('Gamma: ',details[0].lastGreeks.gamma)\n",
    "                # print ('IV: ',details[0].lastGreeks.impliedVol)\n",
    "                # print ('Theta: ',details[0].lastGreeks.theta)\n",
    "\n",
    "                newThings = {'Contract':contract,\n",
    "                            'Trigger':self.curPreds[p][1],\n",
    "                            'Bid':details[0].bid,\n",
    "                            'Ask':details[0].ask,\n",
    "                            'Delta':details[0].lastGreeks.delta,\n",
    "                            'Gamma':details[0].lastGreeks.gamma,\n",
    "                            'IV':details[0].lastGreeks.impliedVol,\n",
    "                            'Theta':details[0].lastGreeks.theta\n",
    "                            }\n",
    "                self.things[sym] = newThings\n",
    "            except:\n",
    "                1+1\n",
    "        d=str(datetime.strptime(datetime.today().strftime('%Y%m%d'), \"%Y%m%d\"))\n",
    "        # save dictionary to person_data.pkl file\n",
    "        with open('./optionData/'+d[:10]+ '_data.pkl', 'wb') as fp:\n",
    "            pickle.dump(self.things, fp)\n",
    "            print('dictionary saved successfully to file')\n",
    "        # self.disconnectIB()\n",
    "        return\n",
    "    \n",
    "    def FindOptionPositions(self):\n",
    "        # self.connectIB()\n",
    "        self.trainDuration = '5 Y'\n",
    "        self.GetCurrentData()\n",
    "        self.predModel()\n",
    "        self.getOptionPositions()\n",
    "\n",
    "    def newOrders(self):\n",
    "        self.trainDuration = '5 Y'\n",
    "        self.GetCurrentData()\n",
    "        self.predModel()\n",
    "        self.getOptionPositions()\n",
    "        self.findBestPositions()\n",
    "        self.placeBracketOrders()\n",
    "        return\n",
    "\n",
    "    def findBestPositions(self):\n",
    "        ''' Returns the top positions for the current predictions & data'''\n",
    "        temp = []\n",
    "        for i in range(len(self.curPreds)):\n",
    "            temp.append(abs(self.curPreds[i][1]))\n",
    "        sort_index = np.argsort(temp)\n",
    "        self.topPicks=[]\n",
    "        for pos in range(self.maxPositions):\n",
    "            self.topPicks.append(self.curPreds[sort_index[-(pos+1)]][0])\n",
    "        return\n",
    "    \n",
    "    def placeBracketOrders(self):\n",
    "        for o in self.topPicks:\n",
    "            try:\n",
    "                eq = self.things[o]\n",
    "                self.ib.qualifyContracts(eq['Contract'])\n",
    "                qty=1\n",
    "                price=eq['Ask']\n",
    "                ProfTaker = float(np.round(self.profTakrRatio*price,1))\n",
    "                StpLs = float(np.round(self.stpLsRatio*price,1))\n",
    "                ticker_bracket_order = self.ib.bracketOrder('BUY',qty,price,ProfTaker,StpLs)\n",
    "                #place bracket order\n",
    "                self.getClosingDate()\n",
    "                for ord in ticker_bracket_order:\n",
    "                    self.ib.sleep(1)\n",
    "                    ord.tif ='GTD'\n",
    "                    ord.goodTillDate = self.GTD\n",
    "                    self.ib.placeOrder(eq['Contract'], ord)\n",
    "            except:\n",
    "                1+1\n",
    "        return\n",
    "    \n",
    "    def getClosingDate(self):\n",
    "    # Get the current date and time\n",
    "        current_date = datetime.now()\n",
    "        # Define a function to add business days\n",
    "        def add_business_days(start_date, num_days):\n",
    "            current_date = start_date\n",
    "            while num_days > 0:\n",
    "                current_date += timedelta(days=1)\n",
    "                if current_date.weekday() < 5:  # Monday (0) to Friday (4) are business days\n",
    "                    num_days -= 1\n",
    "            return current_date\n",
    "\n",
    "        # Calculate the target date with 10 business days added to the current date\n",
    "        target_date = add_business_days(current_date, 11) # 11 days since the trained network is working with 10 days from tomorrow morning!\n",
    "\n",
    "        # Format the current date and target date as strings in the desired format\n",
    "        target_date_string = target_date.strftime(\"%Y%m%d %H:%M:%S\")\n",
    "\n",
    "        self.GTD = target_date_string\n",
    "        return\n",
    "    \n",
    "\n",
    "# Run it\n",
    "if __name__ == \"__main__\":\n",
    "    stock = Stock42()\n",
    "    # stock.CreateModel() # Model Scaffold\n",
    "    # stock.LoadModelFromFile()  # Load Weights\n",
    "    stock.connectIB()  # Connect to Interactive Brokers or your data source\n",
    "    # Get Historic Data\n",
    "    stock.GetCurrentData()\n",
    "    # Get Indiv equity\n",
    "    # stock.getDat('AAPL')  # Replace with the appropriate ticker symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alecjeffery/Documents/Playgrounds/Python/spacetimeformer_stocks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacetimeformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
